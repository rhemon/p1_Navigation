{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# please do not modify the line below\n",
    "# env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")\n",
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64/Banana.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 1e-3               # learning rate \n",
    "UPDATE_EVERY = 5        # how often to update the network\n",
    "ACTOR_LR = 1e-4           # Learning rate for the Actor model\n",
    "CRITIC_LR = 1e-3          # Learning rate for the Critic model\n",
    "WEIGHT_DECAY = 1e-2          # Weight decay (used for Critic)\n",
    "TIMES_UPDATE = 1         # Number of time to learn\n",
    "num_agents = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    \"\"\"\n",
    "    Returns lower and upper limit for the random parameterial initalisation for given\n",
    "    layer.\n",
    "    :param layer: torch's neural network layer\n",
    "    \"\"\"\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"\n",
    "        Initialize critic network.\n",
    "        :param state_size: Number of information provided in the state\n",
    "        :param action_size: Number of actions environment can take\n",
    "        :param seed: Seed for random initialization\n",
    "        \"\"\"\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.critic_layer_1 = nn.Linear(state_size, 64)\n",
    "        self.critic_layer_2 = nn.Linear(64+action_size, 64)\n",
    "        self.critic_layer_3 = nn.Linear(64, 64)\n",
    "        self.critic_out = nn.Linear(64, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Reset parameters initialized for the layers in the network.\n",
    "        \"\"\"\n",
    "        self.critic_layer_1.weight.data.uniform_(*hidden_init(self.critic_layer_1))\n",
    "        self.critic_layer_2.weight.data.uniform_(*hidden_init(self.critic_layer_2))\n",
    "        self.critic_layer_3.weight.data.uniform_(*hidden_init(self.critic_layer_3))\n",
    "        self.critic_out.weight.data.uniform_(-3e-3, 3e-3)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        Feedforward the the network with provided state and action. Returns the Q value.\n",
    "        \n",
    "        :param state: The state of the game\n",
    "        :param action: Action taken by the agent\n",
    "        \"\"\"\n",
    "        x = F.relu(self.critic_layer_1(state))\n",
    "        x = torch.cat([x, action], dim=1)\n",
    "        x = F.relu(self.critic_layer_2(x))\n",
    "        x = F.relu(self.critic_layer_3(x))\n",
    "        return self.critic_out(x)\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"\n",
    "        Initialize critic network.\n",
    "        :param state_size: Number of information provided in the state\n",
    "        :param action_size: Number of actions environment can take\n",
    "        :param seed: Seed for random initialization\n",
    "        \"\"\"\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.actor_layer_1 = nn.Linear(state_size, 64)\n",
    "        self.actor_layer_2 = nn.Linear(64, 64)\n",
    "        self.actor_layer_3 = nn.Linear(64, 64)\n",
    "        self.actor_out = nn.Linear(64, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Reset parameters initialized for the layers in the network.\n",
    "        \"\"\"\n",
    "        self.actor_layer_1.weight.data.uniform_(*hidden_init(self.actor_layer_1))\n",
    "        self.actor_layer_2.weight.data.uniform_(*hidden_init(self.actor_layer_2))\n",
    "        self.actor_layer_3.weight.data.uniform_(*hidden_init(self.actor_layer_3))\n",
    "        self.actor_out.weight.data.uniform_(-3e-3, 3e-3)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Feedforward the the network with provided state. Returns the action values.\n",
    "        \n",
    "        :param state: The state of the game\n",
    "        \"\"\"\n",
    "        x = F.leaky_relu(self.actor_layer_1(state))\n",
    "        x = F.leaky_relu(self.actor_layer_2(x))\n",
    "        x = F.leaky_relu(self.actor_layer_3(x))\n",
    "        return torch.tanh(self.actor_out(x))\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"\n",
    "        Initializes and handles both actor and critic network for the agent.\n",
    "        :param state_size: Number of information provided in the state\n",
    "        :param action_size: Number of actions environment can take\n",
    "        :param seed: Seed for random initialization\n",
    "        \"\"\"\n",
    "        self.seed = seed\n",
    "        self.actor = ActorNetwork(state_size, action_size, seed).to(device)\n",
    "        self.critic = CriticNetwork(state_size, action_size, seed).to(device)\n",
    "\n",
    "    def copy(self, network):\n",
    "        \"\"\"\n",
    "        Copy the parameters from the provided network into current actor critic networks.\n",
    "        :param network: Network instance with actor and critic model initialized\n",
    "        \"\"\"\n",
    "        # Actor\n",
    "        for target_param, local_param in zip(self.actor.parameters(), network.actor.parameters()):\n",
    "            target_param.data.copy_(local_param.data)\n",
    "        \n",
    "        # Critic\n",
    "        for target_param, local_param in zip(self.critic.parameters(), network.critic.parameters()):\n",
    "            target_param.data.copy_(local_param.data)\n",
    "    \n",
    "    def soft_update(self, network, tau=TAU):\n",
    "        \"\"\"\n",
    "        Soft update the current network from the given network.\n",
    "        :param network: Network values that will be used to update current network\n",
    "        :param tau: Floating value to determine how much information goes into current network from provided one.\n",
    "        \"\"\"\n",
    "        # Actor\n",
    "        for target_param, local_param in zip(self.actor.parameters(), network.actor.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "        \n",
    "        # Critic\n",
    "        for target_param, local_param in zip(self.critic.parameters(), network.critic.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, seed=0):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, random_seed, action_low=0, action_high=1):\n",
    "        \"\"\"\n",
    "        Initializes the agent to play in the environment.\n",
    "        \n",
    "        :param state_size: Number of information provided in the state\n",
    "        :param action_size: Number of actions environment can take\n",
    "        :param random_seed: Seed for random initialization\n",
    "        :param action_low: Minimum value for action\n",
    "        :param action_high: Maxmimum value for aciton\n",
    "        \"\"\"\n",
    "        \n",
    "        self.seed = random.seed(random_seed)\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.a_low = action_low\n",
    "        self.a_high = action_high\n",
    "        self.network = Network(state_size, action_size, random_seed)\n",
    "        \n",
    "        self.actor_opt = optim.Adam(self.network.actor.parameters(), lr=ACTOR_LR)\n",
    "        self.critic_opt = optim.Adam(self.network.critic.parameters(), lr=CRITIC_LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        self.target_network = Network(state_size, action_size, random_seed)\n",
    "        \n",
    "        self.ounoise = OUNoise(action_size, action_low, action_high)\n",
    "\n",
    "        self.memory = ReplayBuffer()\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"\n",
    "        Returns action for given state.\n",
    "        :param state: State of the environment,for which to determine an action\n",
    "        :param add_noise: Used to determine whether to add nose based on Ornstein Uhlenbeck process\n",
    "        \"\"\"\n",
    "        state = torch.tensor(state).float().to(device)\n",
    "        self.network.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.network.actor(state)\n",
    "            action = action.data.cpu().numpy()\n",
    "        self.network.actor.train()\n",
    "        if add_noise:\n",
    "            return self.ounoise.get_action(action)\n",
    "        return action\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add the step to memory and if its time to update the model, then train and update the networks.\n",
    "        :param state: State of the environment\n",
    "        :param action: Action taken for that state\n",
    "        :param reward: Reward provided for the taken action\n",
    "        :param next_state: New state that appeared due to action\n",
    "        :param done: Whehter its the terminal state or not\n",
    "        \"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "\n",
    "        if len(self.memory) > BATCH_SIZE and self.t_step == 0:\n",
    "            for i in range(TIMES_UPDATE):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences)\n",
    "                self.target_network.soft_update(self.network)\n",
    "    \n",
    "    def learn(self, experiences, gamma=GAMMA):\n",
    "        \"\"\"\n",
    "        Learning algorithm for the model. Uses the target critic network to determine\n",
    "        the MSE loss for predicted Q values with local network for the experieces sampled \n",
    "        from the memory. In the backpropagation of critic network, clips the gradient values to 1.\n",
    "        Then updates the actor network with the goal to maximize the average value determined by the critic model.\n",
    "        So the loss is -Q_local(state, action).mean(). \n",
    "        :param experiences: State, Actions, Rewards, Next states, dones randomly sampled from the memory\n",
    "        :param gamma: Discount rate that determines how much of future reward impacts total reward.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        Q_target_next = self.target_network.critic(next_states, self.target_network.actor(next_states))\n",
    "        Q_target = rewards + (gamma * Q_target_next * (1-dones))\n",
    "        \n",
    "        Q_predicted = self.network.critic(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_predicted, Q_target)\n",
    "        \n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.critic.parameters(), 1)\n",
    "        self.critic_opt.step()\n",
    "\n",
    "        actor_loss = -self.network.critic(states, self.network.actor(states)).mean()\n",
    "        \n",
    "        self.actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_opt.step()\n",
    "\n",
    "\"\"\"\n",
    "Taken from https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py\n",
    "\"\"\"\n",
    "class OUNoise(object):\n",
    "    def __init__(self, action_size, a_low, a_high, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "        \"\"\"\n",
    "        Initialize the parameters to process the noise values that will be added\n",
    "        to the actions.\n",
    "        \"\"\"\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_size\n",
    "        self.low          = a_low\n",
    "        self.high         = a_high\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset current state of the process\n",
    "        \"\"\"\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        \"\"\"\n",
    "        Update the state\n",
    "        \"\"\"\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def get_action(self, action, t=0): \n",
    "        \"\"\"\n",
    "        For given action add the noise to the action and return it clipped to its range.\n",
    "        \"\"\"\n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return np.clip(action + ou_state, self.low, self.high)\n",
    "\n",
    "def ddpg(agent, env, brain_name, target_score=13, max_t=1000, gamma=GAMMA):\n",
    "    \"\"\"\n",
    "    Training function used to expose the agent to the environment and run through it repeated until\n",
    "    target score is achieved.\n",
    "    :param agent: Agent that will be trained\n",
    "    :param env: Environment the agent will be learning\n",
    "    :param brain_name: Brain name of the environment\n",
    "    :param target_score: Target score the agent needs to achieve\n",
    "    :param max_t: Number of times to iterate each episode\n",
    "    :param gammma: Discount rate\n",
    "    \"\"\"\n",
    "    try:\n",
    "        scores = list(np.load('scores.npz')['scores'])\n",
    "    except:\n",
    "        scores = []\n",
    "    \n",
    "    scores_window = deque(maxlen=100)\n",
    "    i_ep = 0\n",
    "    while True:\n",
    "        i_ep += 1\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        score = np.zeros(num_agents)\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(int(np.argmax(actions)))[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            for i in range(num_agents):\n",
    "                state = states[i]\n",
    "                action = actions[i]\n",
    "                reward = rewards[i]\n",
    "                next_state = next_states[i]\n",
    "                done = dones[i]\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "            score += rewards\n",
    "            states = next_states\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        scores_window.append(score.mean())\n",
    "        scores.append(score.mean())\n",
    "        print('\\rEpisode {} \\tAverage Score: {:.2f}, Max score: {}, Min score: {}'.format(i_ep, np.mean(scores_window), np.max(scores_window), np.min(scores_window)), end=\"\")\n",
    "        np.savez(\"scores.npz\", scores=scores)\n",
    "        if i_ep % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_ep, np.mean(scores_window)))\n",
    "            torch.save(agent.network.actor.state_dict(), f'ddpg_actor_checkpoint.pth') \n",
    "            torch.save(agent.network.critic.state_dict(), f'ddpg_critic_checkpoint.pth')\n",
    "\n",
    "        if np.mean(scores_window) > target_score and i_ep > 100:\n",
    "            print('\\rSolved goal on episode {} with average score {}'.format(i_ep, np.mean(scores_window)))\n",
    "            torch.save(agent.network.actor.state_dict(), f'ddpg_actor_solution.pth') \n",
    "            torch.save(agent.network.critic.state_dict(), f'ddpg_critic_solution.pth')\n",
    "            break\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.122, Max score: 4.0, Min score: -3.0\n",
      "Episode 200\tAverage Score: -0.099, Max score: 3.0, Min score: -3.0\n",
      "Episode 300\tAverage Score: 0.000, Max score: 5.0, Min score: -5.00\n",
      "Episode 400\tAverage Score: 0.144, Max score: 4.0, Min score: -4.00\n",
      "Episode 463 \tAverage Score: -0.12, Max score: 4.0, Min score: -4.0"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size, action_size, 1)\n",
    "scores = ddpg(agent, env, brain_name, max_t=1000)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.load(\"scores.npz\")[\"scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
